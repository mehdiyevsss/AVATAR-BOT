<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Voice Avatar Chatbot</title>
  <style>
    * { box-sizing: border-box; }

    html, body {
      margin: 0;
      padding: 0;
      height: 100vh;
      width: 100vw;
      font-family: "Segoe UI", sans-serif;
      background: #574e4e;
      color: #2D3439;
      display: flex;
      justify-content: center;
      align-items: center;
    }

    .container {
      display: flex;
      height: 90vh;
      width: 90vw;
      max-width: 1400px;
      border-radius: 16px;
      background: #fff;
      box-shadow: 0 2px 8px rgba(225, 228, 232, 0.5);
      overflow: hidden;
    }

    #avatar {
      flex: 1 1 0;
      height: 100%;
      background: url('/static/Swisscom.jpg') center center / cover no-repeat;
    }


    .chat-section {
      flex: 1 1 0;
      display: flex;
      flex-direction: column;
      padding: 20px;
      background: #f5f5f5;
      height: 100%;
    }

    .messages {
      flex: 1;
      overflow-y: auto;
      margin-bottom: 15px;
      padding-right: 10px;
      display: flex;
      flex-direction: column;
    }

    .message {
      margin-bottom: 10px;
      padding: 14px 18px;
      border-radius: 14px;
      font-size: 15px;
      max-width: 100%;
      background: #e0e0e0;
      color: #000;
    }

    .assistant {
      align-self: flex-start;
      background-color: #dbe4ff;
    }

    .user {
      align-self: flex-end;
      background-color: #ffe2e2;
    }

    .system {
      align-self: center;
      background-color: #fff3cd;
      color: #856404;
      text-align: center;
    }

    .controls {
      display: flex;
      gap: 10px;
      flex-wrap: wrap;
    }

    .record-btn,
    .human-btn,
    .video-btn {
      padding: 14px;
      font-size: 16px;
      border-radius: 10px;
      border: none;
      cursor: pointer;
      transition: background 0.3s;
      display: flex;
      align-items: center;
      justify-content: center;
      flex: 1;
      min-width: 120px;
    }

    .record-btn {
      background: #333;
      color: white;
    }

    .human-btn {
      background: #28a745;
      color: white;
    }

    .video-btn {
      background: #007bff;
      color: white;
    }

    .record-btn:disabled,
    .human-btn:disabled,
    .video-btn:disabled {
      background: #666;
      cursor: not-allowed;
    }

    .spinner {
      width: 16px;
      height: 16px;
      margin-left: 10px;
      border: 2px solid white;
      border-top: 2px solid transparent;
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }

    @keyframes spin {
      to { transform: rotate(360deg); }
    }

    .video-container {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.9);
      z-index: 1000;
      flex-direction: column;
      align-items: center;
      justify-content: center;
    }

    .video-container.active {
      display: flex;
    }

    .video-grid {
      display: flex;
      gap: 20px;
      margin-bottom: 20px;
    }

    .video-box {
      background: #333;
      border-radius: 10px;
      overflow: hidden;
      position: relative;
    }

    .local-video {
      width: 300px;
      height: 225px;
    }

    .remote-video {
      width: 600px;
      height: 450px;
    }

    .video-box video {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    .video-label {
      position: absolute;
      bottom: 10px;
      left: 10px;
      background: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 5px 10px;
      border-radius: 5px;
      font-size: 12px;
    }

    .video-controls {
      display: flex;
      gap: 10px;
    }

    .video-controls button {
      padding: 12px 20px;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-size: 14px;
      transition: background 0.3s;
    }

    .end-call {
      background: #dc3545;
      color: white;
    }

    .toggle-video,
    .toggle-audio {
      background: #6c757d;
      color: white;
    }

    .connection-status {
      margin-bottom: 10px;
      padding: 10px;
      border-radius: 5px;
      text-align: center;
      font-weight: bold;
    }

    .connecting {
      background: #fff3cd;
      color: #856404;
    }

    .connected {
      background: #d4edda;
      color: #155724;
    }

    .disconnected {
      background: #f8d7da;
      color: #721c24;
    }
  </style>

  <script type="importmap">
    {
      "imports": {
        "three": "https://cdn.jsdelivr.net/npm/three@0.170.0/build/three.module.js/+esm",
        "three/addons/": "https://cdn.jsdelivr.net/npm/three@0.170.0/examples/jsm/",
        "talkinghead": "https://cdn.jsdelivr.net/gh/met4citizen/TalkingHead@1.5/modules/talkinghead.mjs"
      }
    }
  </script>
</head>

<body>
  <div class="container">
    <div id="avatar"></div>

    <div class="chat-section">
      <div class="messages" id="messages"></div>
      <div class="controls">
        <button id="recordBtn" class="record-btn">üéôÔ∏è Record</button>
        <button id="humanBtn" class="human-btn" style="display: none;">üë• Connect to Human</button>
      </div>
    </div>
  </div>

  <!-- Video Call Container (unchanged) -->
  <div class="video-container" id="videoContainer">
    <div class="connection-status" id="connectionStatus">Connecting to operator...</div>

    <div class="video-grid">
      <div class="video-box local-video">
        <video id="localVideo" autoplay muted></video>
        <div class="video-label">You</div>
      </div>

      <div class="video-box remote-video">
        <video id="remoteVideo" autoplay></video>
        <div class="video-label">Operator</div>
      </div>
    </div>

    <div class="video-controls">
      <button id="toggleVideo" class="toggle-video">üìπ Video On</button>
      <button id="toggleAudio" class="toggle-audio">üé§ Audio On</button>
      <button id="endCall" class="end-call">üìû End Call</button>
    </div>
  </div>
 
  <!-- Model Viewer -->
  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
  
  <!-- Main Script -->
  <script type="module">
    import { TalkingHead } from "talkinghead";

    let head;
    const avatarContainer = document.getElementById("avatar");

    async function initAvatar() {
      // === constructor now includes lipsyncModules & lipsyncLang ===
      head = new TalkingHead(avatarContainer, {
        ttsEndpoint: "none",
        lipsyncModules: ["en"],    // ‚Üê load English viseme module
        lipsyncLang: "en",         // ‚Üê drive lip-sync in English
        cameraView: "full",
        debug: true  // Enable debug to see emotion changes
      });
      
      await head.showAvatar({
        url: 'https://models.readyplayer.me/685addbcb695ba2bdaf8c7a3.glb?morphTargets=ARKit,Oculus+Visemes,mouthOpen,mouthSmile,eyesClosed,eyesLookUp,eyesLookDown&textureSizeLimit=1024&textureFormat=png',
        body: 'M',
        avatarMood: 'neutral'
      });
      
      // Inspect the avatar structure to find bones
      console.log('Avatar loaded, inspecting structure...');
      setTimeout(() => {
        inspectAvatarStructure();
      }, 1000);
      
      // Initialize with a greeting and happy emotion
      setTimeout(() => {
        changeEmotion('happy');
        // Perform a welcome animation after initialization
        setTimeout(() => {
          try {
            triggerAnimation('wave');
          } catch (error) {
            console.log('Welcome animation error:', error);
          }
        }, 1500); // Give more time for the avatar to fully load
      }, 2000); // Give more time for initialization
    }

    const recordBtn = document.getElementById("recordBtn");
    const humanBtn  = document.getElementById("humanBtn");
    const messagesEl = document.getElementById("messages");

    function addMessage(role, text) {
      const div = document.createElement("div");
      div.className = `message ${role}`;
      div.textContent = text;
      messagesEl.appendChild(div);
      messagesEl.scrollTop = messagesEl.scrollHeight;
      
      // Set emotion based on message content if it's from the assistant
      if (role === 'assistant' && head) {
        detectAndSetEmotion(text);
      }
    }
    
    // Variable to track emotion reset timer
    let emotionResetTimer = null;
    
    // Function to change avatar's emotion
    function changeEmotion(emotion) {
      if (!head) return;
      
      // Only use emotions we know are supported
      const supportedEmotions = ['neutral', 'happy', 'sad', 'angry'];
      if (!supportedEmotions.includes(emotion)) {
        emotion = 'neutral';
      }
      
      console.log(`Changing emotion to: ${emotion}`);
      try {
        head.setMood(emotion);
        
        // Clear any existing timer
        if (emotionResetTimer) {
          clearTimeout(emotionResetTimer);
          emotionResetTimer = null;
        }
        
        // Set a timer to reset to neutral after a delay, but only for non-neutral emotions
        if (emotion !== 'neutral') {
          const resetDelay = emotion === 'angry' ? 5000 : 8000; // Reset angry faster than other emotions
          emotionResetTimer = setTimeout(() => {
            console.log('Resetting emotion to neutral after timeout');
            try {
              head.setMood('neutral');
            } catch (e) {
              console.log('Failed to reset mood to neutral:', e);
            }
          }, resetDelay);
        }
      } catch (error) {
        console.log('Error setting mood:', error);
        // Fall back to neutral if there's an error
        if (emotion !== 'neutral') {
          try {
            head.setMood('neutral');
          } catch (e) {
            console.log('Failed to set neutral mood:', e);
          }
        }
      }
    }
    
    // Function to detect emotion from text and trigger animations
    function detectAndSetEmotion(text) {
      // Simple keyword-based emotion detection - only using supported emotions
      const emotions = {
        happy: ['happy', 'glad', 'joy', 'exciting', 'excited', 'wonderful', 'great', 'üòä', 'üòÉ', 'thank you'],
        sad: ['sad', 'sorry', 'unfortunate', 'regret', 'unhappy', 'bad news', 'üòî', 'üò¢'],
        angry: ['angry', 'upset', 'frustrating', 'annoyed', 'annoying', 'üò†', 'üò°', 'not working', "isn't working", "can't believe", 'terrible', 'awful', 'horrible', 'hate', 'mad', 'furious'],
        neutral: ['neutral', 'normal', 'standard']
      };
      
      // Log the text for debugging
      console.log('Detecting emotion from text:', text);
      
      // Animation triggers for various gestures
      const nodTriggers = ['yes', 'agree', 'correct', 'right', 'good', 'exactly', 'absolutely', 'sure', 'definitely'];
      const waveTriggers = ['hello', 'hi', 'hey', 'greetings', 'welcome', 'goodbye', 'bye', 'see you', 'welcome'];
      const thumbsUpTriggers = ['awesome', 'excellent', 'perfect', 'great job', 'well done', 'nice', 'cool', 'fantastic', 'brilliant'];
      const pointTriggers = ['look', 'there', 'that one', 'this one', 'check', 'see that', 'over there', 'direction'];
      const shakeTriggers = ['no', 'nope', 'disagree', 'incorrect', 'wrong', 'not right', 'never', 'don\'t'];
      const shrugTriggers = ['maybe', 'perhaps', 'not sure', 'uncertain', 'who knows', 'whatever', 'dunno', 'don\'t know'];
      
      const textLower = text.toLowerCase();
      let detectedEmotion = 'neutral';
      
      // Check for question - use happy instead of surprised which may not be supported
      if (textLower.includes('?')) {
        detectedEmotion = 'happy';
      }
      
      // First check for angry emotion specifically to prioritize it
      const angryKeywords = emotions.angry;
      if (angryKeywords.some(keyword => textLower.includes(keyword))) {
        detectedEmotion = 'angry';
        console.log('Angry emotion detected from keywords:', angryKeywords.filter(keyword => textLower.includes(keyword)));
      } 
      // Only check for other emotions if angry wasn't detected
      else {
        // Check for other emotions
        for (const [emotion, keywords] of Object.entries(emotions)) {
          if (emotion !== 'angry' && keywords.some(keyword => textLower.includes(keyword))) {
            detectedEmotion = emotion;
            console.log(`${emotion} emotion detected from keywords:`, keywords.filter(keyword => textLower.includes(keyword)));
            break;
          }
        }
      }
      
      changeEmotion(detectedEmotion);
      
      // Check for animation triggers
      console.log('Checking for animation triggers in:', textLower);
      
      if (nodTriggers.some(trigger => textLower.includes(trigger))) {
        console.log('NOD TRIGGER DETECTED:', nodTriggers.filter(trigger => textLower.includes(trigger)));
        triggerAnimation('nod');
      } else if (waveTriggers.some(trigger => textLower.includes(trigger))) {
        console.log('WAVE TRIGGER DETECTED:', waveTriggers.filter(trigger => textLower.includes(trigger)));
        triggerAnimation('wave');
      } else if (thumbsUpTriggers.some(trigger => textLower.includes(trigger))) {
        console.log('THUMBS UP TRIGGER DETECTED:', thumbsUpTriggers.filter(trigger => textLower.includes(trigger)));
        triggerAnimation('thumbsUp');
      } else if (pointTriggers.some(trigger => textLower.includes(trigger))) {
        console.log('POINT TRIGGER DETECTED:', pointTriggers.filter(trigger => textLower.includes(trigger)));
        triggerAnimation('point');
      } else if (shakeTriggers.some(trigger => textLower.includes(trigger))) {
        console.log('SHAKE TRIGGER DETECTED:', shakeTriggers.filter(trigger => textLower.includes(trigger)));
        triggerAnimation('shake');
      } else if (shrugTriggers.some(trigger => textLower.includes(trigger))) {
        console.log('SHRUG TRIGGER DETECTED:', shrugTriggers.filter(trigger => textLower.includes(trigger)));
        triggerAnimation('shrug');
      } else {
        console.log('No animation triggers detected in text');
      }
    }
    
    // Function to inspect the avatar structure and find bones
    function inspectAvatarStructure() {
      if (!head) {
        console.log('Avatar not fully loaded yet');
        return;
      }
      
      console.log('Inspecting avatar structure:');
      
      // Inspect the TalkingHead object directly
      console.log('TalkingHead object:', head);
      
      // Check what properties and methods are available
      console.log('Available properties and methods:');
      for (const prop in head) {
        console.log(`- ${prop}: ${typeof head[prop]}`);
      }
      
      // Try to access the scene or model
      if (head.scene) {
        console.log('Scene found:', head.scene);
      }
      
      if (head.model) {
        console.log('Model found:', head.model);
      }
      
      // Check if there's a getObject or similar method
      if (typeof head.getObject === 'function') {
        console.log('getObject method found, trying to use it...');
        try {
          const object = head.getObject();
          console.log('Retrieved object:', object);
        } catch (error) {
          console.log('Error using getObject:', error);
        }
      }
      
      // Try to access any exposed bones or animation system
      if (head.bones) {
        console.log('Bones found:', head.bones);
      }
      
      if (head.animator) {
        console.log('Animator found:', head.animator);
      }
      
      // Try to access any Three.js objects that might be exposed
      console.log('Looking for Three.js objects...');
      ['three', 'THREE', 'threeJS', 'renderer', 'camera'].forEach(prop => {
        if (head[prop]) {
          console.log(`Found ${prop}:`, head[prop]);
        }
      });
      
      // Check if we can access the avatar's source code
      console.log('TalkingHead prototype:', Object.getPrototypeOf(head));
      
      // Try to find any methods related to animation or gestures
      const animationMethods = Object.getOwnPropertyNames(Object.getPrototypeOf(head))
        .filter(method => 
          typeof head[method] === 'function' && 
          (method.toLowerCase().includes('anim') || 
           method.toLowerCase().includes('move') || 
           method.toLowerCase().includes('gesture') || 
           method.toLowerCase().includes('pose'))
        );
      
      console.log('Animation-related methods:', animationMethods);
    }
    
    // Function to trigger avatar animations using TalkingHead's built-in methods and properties
    function triggerAnimation(animationType) {
      console.log(`Animation triggered: ${animationType}`);
      
      if (!head) {
        console.error('Cannot trigger animation: head object not available');
        return;
      }
      
      // Log available objects for debugging
      console.log('Available animation objects:', {
        head: head,
        objectHead: head.objectHead ? 'Available' : 'Not available',
        objectRightArm: head.objectRightArm ? 'Available' : 'Not available',
        objectLeftArm: head.objectLeftArm ? 'Available' : 'Not available',
        lookAt: typeof head.lookAt === 'function' ? 'Available' : 'Not available'
      });
      
      try {
        if (animationType === 'nod') {
          // Use simple head movements for nodding
          const nodAnimation = async () => {
            // Use the TalkingHead's built-in lookAt method
            head.lookAt(0, -0.5, 1); // Look down slightly
            await new Promise(r => setTimeout(r, 300));
            head.lookAt(0, 0, 1);    // Look straight
            await new Promise(r => setTimeout(r, 200));
            head.lookAt(0, -0.5, 1); // Look down slightly
            await new Promise(r => setTimeout(r, 300));
            head.lookAt(0, 0, 1);    // Back to normal
          };
          nodAnimation();
        } else if (animationType === 'shake') {
          // Head shake animation (no)
          const shakeAnimation = async () => {
            const headObj = head.objectHead;
            let originalRotation = null;
            
            if (headObj) {
              // Store original rotation
              originalRotation = {
                x: headObj.rotation.x,
                y: headObj.rotation.y,
                z: headObj.rotation.z
              };
              
              // Shake head left and right
              for (let i = 0; i < 3; i++) {
                // Shake right
                for (let j = 0; j < 5; j++) {
                  headObj.rotation.y -= 0.06;
                  await new Promise(r => setTimeout(r, 20));
                }
                
                // Shake left
                for (let j = 0; j < 10; j++) {
                  headObj.rotation.y += 0.06;
                  await new Promise(r => setTimeout(r, 20));
                }
                
                // Back to center
                for (let j = 0; j < 5; j++) {
                  headObj.rotation.y -= 0.06;
                  await new Promise(r => setTimeout(r, 20));
                }
              }
              
              // Return to original position
              headObj.rotation.x = originalRotation.x;
              headObj.rotation.y = originalRotation.y;
              headObj.rotation.z = originalRotation.z;
            } else {
              // Fallback using lookAt if head object isn't available
              head.lookAt(-0.5, 0, 1);
              await new Promise(r => setTimeout(r, 150));
              head.lookAt(0.5, 0, 1);
              await new Promise(r => setTimeout(r, 150));
              head.lookAt(-0.5, 0, 1);
              await new Promise(r => setTimeout(r, 150));
              head.lookAt(0.5, 0, 1);
              await new Promise(r => setTimeout(r, 150));
              head.lookAt(0, 0, 1);
            }
          };
          shakeAnimation();
        } else if (animationType === 'thumbsUp') {
          // Thumbs up animation
          const thumbsUpAnimation = async () => {
            const rightArm = head.objectRightArm;
            
            if (rightArm) {
              // Store original rotation
              const originalRotation = {
                x: rightArm.rotation.x,
                y: rightArm.rotation.y,
                z: rightArm.rotation.z
              };
              
              // Also animate the head slightly
              const headObj = head.objectHead;
              const originalHeadRotation = headObj ? {
                x: headObj.rotation.x,
                y: headObj.rotation.y,
                z: headObj.rotation.z
              } : null;
              
              // Slight smile/nod
              if (headObj) {
                headObj.rotation.x -= 0.1;
              }
              
              // Raise arm to thumbs up position
              for (let i = 0; i < 10; i++) {
                rightArm.rotation.z += 0.04;
                rightArm.rotation.x -= 0.02;
                await new Promise(r => setTimeout(r, 20));
              }
              
              // Hold the thumbs up for a moment
              await new Promise(r => setTimeout(r, 800));
              
              // Add a slight emphasis movement
              for (let i = 0; i < 2; i++) {
                // Up motion
                for (let j = 0; j < 5; j++) {
                  rightArm.rotation.z += 0.02;
                  await new Promise(r => setTimeout(r, 20));
                }
                
                // Down motion
                for (let j = 0; j < 5; j++) {
                  rightArm.rotation.z -= 0.02;
                  await new Promise(r => setTimeout(r, 20));
                }
              }
              
              // Lower arm gradually
              for (let i = 0; i < 10; i++) {
                rightArm.rotation.z -= 0.04;
                rightArm.rotation.x += 0.02;
                await new Promise(r => setTimeout(r, 30));
              }
              
              // Return to original positions
              if (headObj && originalHeadRotation) {
                headObj.rotation.x = originalHeadRotation.x;
                headObj.rotation.y = originalHeadRotation.y;
                headObj.rotation.z = originalHeadRotation.z;
              }
              
              rightArm.rotation.x = originalRotation.x;
              rightArm.rotation.y = originalRotation.y;
              rightArm.rotation.z = originalRotation.z;
            } else {
              // Fallback if arm object isn't available - just nod
              head.lookAt(0, -0.3, 1);
              await new Promise(r => setTimeout(r, 300));
              head.lookAt(0, 0, 1);
            }
          };
          thumbsUpAnimation();
        } else if (animationType === 'wave') {
          console.log('Wave animation triggered');
          
          // Simple fallback - just use lookAt for a greeting motion
          // This is a very basic greeting gesture that should work on any avatar
          const waveAnimation = async () => {
            try {
              // Look at user and nod slightly
              head.lookAt(0, 0, 1);
              await new Promise(r => setTimeout(r, 300));
              
              // Look to the side (as if waving)
              head.lookAt(0.5, 0, 1);
              await new Promise(r => setTimeout(r, 300));
              
              // Look back at user
              head.lookAt(0, 0, 1);
              await new Promise(r => setTimeout(r, 300));
              
              console.log('Simple wave animation completed');
            } catch (error) {
              console.error('Error during wave animation:', error);
            }
          };
          
          waveAnimation();
          return;
        } else {
          // Third approach: Try to use pose templates if available
          if (head.poseTemplates && typeof head.setPose === 'function') {
            console.log('Trying to use pose templates');
            // Check if there's a greeting or wave pose
            const possiblePoses = ['wave', 'greeting', 'hello', 'raise_hand'];
            
            for (const pose of possiblePoses) {
              if (head.poseTemplates[pose]) {
                console.log(`Using ${pose} pose`);
                head.setPose(pose);
                
                // Reset pose after a few seconds
                setTimeout(() => {
                  head.setPose('default');
                }, 2000);
                
                return;
              }
            }
          }
          
          // Fallback: Use head movements as before
          console.log('Using fallback head movement for wave');
          const waveAnimation = async () => {
            try {
              // Look to the side slightly
              head.lookAt(0.5, 0, 1);
              await new Promise(r => setTimeout(r, 200));
              // Look up slightly
              head.lookAt(0.5, 0.3, 1);
              await new Promise(r => setTimeout(r, 200));
              // Look down slightly
              head.lookAt(0.5, -0.3, 1);
              await new Promise(r => setTimeout(r, 200));
              // Look up slightly again
              head.lookAt(0.5, 0.3, 1);
              await new Promise(r => setTimeout(r, 200));
              // Back to normal
              head.lookAt(0, 0, 1);
            } catch (error) {
              console.error('Error during fallback wave animation:', error);
            }
          };
          waveAnimation();
        }
      } catch (error) {
        console.log('Animation error:', error);
      }
    }

    // Track if we're currently recording
    let isRecording = false;
    let currentWs = null;
    
    recordBtn.onclick = async () => {
      if (!head) {
        await initAvatar();
      }

      // If already recording, do nothing (button is disabled during recording)
      if (isRecording) return;
      
      // stop any playing TTS
      if (head.currentAudio && !head.currentAudio.paused) {
        head.currentAudio.pause();
      }
      
      // Update button to show we're listening
      isRecording = true;
      recordBtn.textContent = "üé§ Listening...";
      recordBtn.style.backgroundColor = "#dc3545"; // Red background while recording

      const ws = new WebSocket(
        `${window.location.protocol === 'https:' ? 'wss' : 'ws'}://` +
        `${window.location.host}/ws/audio`
      );
      
      currentWs = ws;

      ws.onmessage = async ({ data }) => {
        const { transcript, response, audio_url, needs_human_operator } = JSON.parse(data);
        addMessage("user", transcript);
        
        // Set a neutral emotion while processing
        if (head) changeEmotion('neutral');
        
        // Short delay before showing response
        setTimeout(() => {
          addMessage("assistant", response);
          
          // Reset the button after response is received
          isRecording = false;
          recordBtn.textContent = "üéôÔ∏è Record";
          recordBtn.style.backgroundColor = "#333"; // Reset to original color
          
          // Close the WebSocket connection so user needs to press record again for next question
          if (ws && ws.readyState === WebSocket.OPEN) {
            ws.close();
          }
        }, 300);

        if (needs_human_operator) {
          humanBtn.style.display = "block";
          addMessage("system", "Please click the button below to connect with a human operator.");
        }

        const blob = await fetch(audio_url).then(r => r.blob());
        const buffer = await blob.arrayBuffer();
        const decoded = await head.audioCtx.decodeAudioData(buffer);

        // build timed word arrays
        const whisperResp = await fetch("https://api.openai.com/v1/audio/transcriptions", {
          method: "POST",
          body: (() => {
            const f = new FormData();
            f.append("file", new File([blob], "tts.mp3", { type: "audio/mpeg" }));
            f.append("model", "whisper-1");
            f.append("language", "en");
            f.append("response_format", "verbose_json");
            f.append("prompt", "[The following is a full verbatim transcription without additional details, comments or emojis:]");
            f.append("timestamp_granularities[]", "word");
            return f;
          })(),
          headers: { "Authorization": "Bearer API-KEY-HERE" }
        });

        const json = await whisperResp.json();
        const timed = {
          audio: decoded,
          words: [],
          wtimes: [],
          wdurations: [],
          markers: [],
          mtimes: []
        };

        json.words.forEach(w => {
          timed.words.push(w.word);
          // convert seconds‚Üíms and offset a bit for early mouth opening
          timed.wtimes.push(w.start * 1000 - 150);
          timed.wdurations.push((w.end - w.start) * 1000);
        });

        head.speakAudio(timed);
      };

      // start streaming mic
      navigator.mediaDevices.getUserMedia({ audio: true })
        .then(stream => {
          const recorder = new MediaRecorder(stream, { mimeType: 'audio/webm' });
          recorder.ondataavailable = e => {
            if (ws.readyState === WebSocket.OPEN) ws.send(e.data);
          };
          recorder.start(250);
          ws.onclose = () => recorder.stop();
        })
        .catch(err => console.error("Mic error:", err));
    };

    function detectSilence(threshold = 0.01, timeout = 1500) {
      const data = new Uint8Array(analyserInput.fftSize);
      let lastActivity = Date.now();
      (function loop(){
        analyserInput.getByteTimeDomainData(data);
        const silent = data.every(v=>Math.abs(v-128) < threshold*128);
        if (!silent) lastActivity = Date.now();
        if (Date.now() - lastActivity > timeout && mediaRecorder.state==="recording") {
          mediaRecorder.stop();
        } else {
          requestAnimationFrame(loop);
        }
      })();
    }

    function showLoading(loading) {
      if (loading) {
        recordBtn.disabled = true;
        recordBtn.innerHTML = `Listening... <span class="spinner"></span>`;
      } else {
        recordBtn.disabled = false;
        recordBtn.textContent = "üéôÔ∏è Start";
      }
    }

    // ====== RTC BUTTONS & ELEMENTS ======
    const videoContainer    = document.getElementById("videoContainer");
    const connectionStatus  = document.getElementById("connectionStatus");
    const localVideo        = document.getElementById("localVideo");
    const remoteVideo       = document.getElementById("remoteVideo");
    const toggleVideoBtn    = document.getElementById("toggleVideo");
    const toggleAudioBtn    = document.getElementById("toggleAudio");
    const endCallBtn        = document.getElementById("endCall");

    // ====== UTILS ======
    function generateUUID() {
      return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'
        .replace(/[xy]/g, c => (Math.random()*16|0).toString(16));
    }

    // ====== HUMAN BUTTON ======
    humanBtn.onclick = initializeVideoCall;

    // ====== WEBRTC STATE ======
    let localStream, peerConnection, signalingSocket;
    const clientId = generateUUID();
    let isVideoEnabled = true, isAudioEnabled = true;

    // ====== INITIALIZE VIDEO CALL ======
    async function initializeVideoCall() {
      try {
        localStream = await navigator.mediaDevices.getUserMedia({ video:true, audio:true });
        localVideo.srcObject = localStream;
        videoContainer.classList.add('active');
        setupPeerConnection();
        connectToSignalingServer();
        addMessage("system", "Connecting you to a human operator...");
      } catch (err) {
        console.error(err);
        addMessage("system", "Unable to access camera/microphone.");
      }
    }

    // ====== PEER CONNECTION ======
    function setupPeerConnection() {
      peerConnection = new RTCPeerConnection({
        iceServers: [
          { urls: 'stun:stun.l.google.com:19302' },
          { urls: 'stun:stun1.l.google.com:19302' },
          { urls: 'stun:stun2.l.google.com:19302' }
        ]
      });
      localStream.getTracks().forEach(track =>
        peerConnection.addTrack(track, localStream)
      );
      peerConnection.ontrack = e => {
        remoteVideo.srcObject = e.streams[0];
        connectionStatus.textContent = "Connected to operator";
        connectionStatus.className = "connection-status connected";
      };
      peerConnection.onicecandidate = e => {
        if (e.candidate && signalingSocket.readyState===WebSocket.OPEN) {
          signalingSocket.send(JSON.stringify({ type:'ice-candidate', candidate:e.candidate }));
        }
      };
      peerConnection.onconnectionstatechange = () => {
        const s = peerConnection.connectionState;
        if (s==='disconnected' || s==='failed') {
          connectionStatus.textContent = "Connection lost";
          connectionStatus.className = "connection-status disconnected";
          setTimeout(endVideoCall,2000);
        }
      };
    }

    // ====== SIGNALING ======
    function connectToSignalingServer() {
      const WS_BASE = window.location.protocol==='https:'?'wss:':'ws:';
      signalingSocket = new WebSocket(`${WS_BASE}//${window.location.host}/ws/signaling/${clientId}/customer`);
      signalingSocket.onopen = () => {
        connectionStatus.textContent = "Waiting for operator...";
        connectionStatus.className = "connection-status connecting";
      };
      signalingSocket.onmessage = async ({data}) => {
        const msg = JSON.parse(data);
        switch(msg.type) {
          case 'matched':
            connectionStatus.textContent = "Operator found! Establishing connection...";
            const offer = await peerConnection.createOffer({ offerToReceiveAudio:true, offerToReceiveVideo:true });
            await peerConnection.setLocalDescription(offer);
            signalingSocket.send(JSON.stringify({ type:'offer', offer }));
            break;
          case 'answer':
            await peerConnection.setRemoteDescription(msg.answer);
            break;
          case 'ice-candidate':
            await peerConnection.addIceCandidate(msg.candidate);
            break;
          case 'partner_disconnected':
            connectionStatus.textContent = "Operator disconnected";
            connectionStatus.className = "connection-status disconnected";
            setTimeout(endVideoCall,2000);
            break;
        }
      };
      signalingSocket.onerror = e => {
        console.error(e);
        connectionStatus.textContent = "Connection error";
        connectionStatus.className = "connection-status disconnected";
      };
      signalingSocket.onclose = () => {
        if (videoContainer.classList.contains('active')) {
          connectionStatus.textContent = "Connection lost";
          connectionStatus.className = "connection-status disconnected";
        }
      };
    }

    // ====== VIDEO CONTROLS ======
    toggleVideoBtn.onclick = () => {
      isVideoEnabled = !isVideoEnabled;
      localStream.getVideoTracks().forEach(t=>t.enabled=isVideoEnabled);
      toggleVideoBtn.textContent = isVideoEnabled?"üìπ Video On":"üìπ Video Off";
    };
    toggleAudioBtn.onclick = () => {
      isAudioEnabled = !isAudioEnabled;
      localStream.getAudioTracks().forEach(t=>t.enabled=isAudioEnabled);
      toggleAudioBtn.textContent = isAudioEnabled?"üé§ Audio On":"üé§ Audio Off";
    };
    
    endCallBtn.onclick = endVideoCall;

    function endVideoCall() {
      if (peerConnection) { peerConnection.close(); peerConnection=null; }
      if (signalingSocket && signalingSocket.readyState===WebSocket.OPEN) {
        signalingSocket.send(JSON.stringify({type:'disconnect'}));
        signalingSocket.close();
      }
      if (localStream) {
        localStream.getTracks().forEach(t=>t.stop());
        localStream=null;
      }
      videoContainer.classList.remove('active');
      humanBtn.style.display = "none";
      addMessage("system", "Video call ended. You can continue chatting.");
    }

    // ====== CLEANUP ON UNLOAD ======
    window.addEventListener('beforeunload', () => {
      if (peerConnection||signalingSocket) endVideoCall();
    });

    initAvatar(); 
    
    // No emotion testing buttons - emotions will work automatically based on conversation
    
  </script>
</body>
</html>