<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Voice Avatar Chatbot</title>
  <style>
    * {
      box-sizing: border-box;
    }

    html,
    body {
      margin: 0;
      padding: 0;
      height: 100vh;
      width: 100vw;
      font-family: "Segoe UI", sans-serif;
      background: #574e4e;
      color: #2D3439;
      display: flex;
      justify-content: center;
      align-items: center;
    }

    .container {
      display: flex;
      height: 90vh;
      width: 90vw;
      max-width: 1400px;
      border-radius: 16px;
      background: #fff;
      box-shadow: 0 2px 8px rgba(225, 228, 232, 0.5);
      overflow: hidden;
    }

    model-viewer {
      flex: 1 1 0;
      height: 100%;
      background: 
    url('/static/Swisscom.jpg')   /* path to your JPEG */
    center center            /* position */
    / cover no-repeat;  
      background-color: #000;
    } 

    .chat-section {
      flex: 1 1 0;
      display: flex;
      flex-direction: column;
      padding: 20px;
      background: #f5f5f5;
      height: 100%;
    }

    .messages {
      flex: 1;
      overflow-y: auto;
      margin-bottom: 15px;
      padding-right: 10px;
      display: flex;
      flex-direction: column;
    }

    .message {
      margin-bottom: 10px;
      padding: 14px 18px;
      border-radius: 14px;
      font-size: 15px;
      max-width: 100%;
      background: #e0e0e0;
      color: #000;
    }

    .assistant {
      align-self: flex-start;
      background-color: #dbe4ff;
    }

    .user {
      align-self: flex-end;
      background-color: #ffe2e2;
    }

    .system {
      align-self: center;
      background-color: #fff3cd;
      color: #856404;
      text-align: center;
    }

    .controls {
      display: flex;
      gap: 10px;
      flex-wrap: wrap;
    }

    .record-btn,
    .human-btn,
    .video-btn {
      padding: 14px;
      font-size: 16px;
      border-radius: 10px;
      border: none;
      cursor: pointer;
      transition: background 0.3s;
      display: flex;
      align-items: center;
      justify-content: center;
      flex: 1;
      min-width: 120px;
    }

    .record-btn {
      background: #333;
      color: white;
    }

    .human-btn {
      background: #28a745;
      color: white;
    }

    .video-btn {
      background: #007bff;
      color: white;
    }

    .record-btn:disabled,
    .human-btn:disabled,
    .video-btn:disabled {
      background: #666;
      cursor: not-allowed;
    }

    .spinner {
      width: 16px;
      height: 16px;
      margin-left: 10px;
      border: 2px solid white;
      border-top: 2px solid transparent;
      border-radius: 50%;
      animation: spin 1s linear infinite;
    }

    @keyframes spin {
      to {
        transform: rotate(360deg);
      }
    }

    .video-container {
      display: none;
      position: fixed;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(0, 0, 0, 0.9);
      z-index: 1000;
      flex-direction: column;
      align-items: center;
      justify-content: center;
    }

    .video-container.active {
      display: flex;
    }

    .video-grid {
      display: flex;
      gap: 20px;
      margin-bottom: 20px;
    }

    .video-box {
      background: #333;
      border-radius: 10px;
      overflow: hidden;
      position: relative;
    }

    .local-video {
      width: 300px;
      height: 225px;
    }

    .remote-video {
      width: 600px;
      height: 450px;
    }

    .video-box video {
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    .video-label {
      position: absolute;
      bottom: 10px;
      left: 10px;
      background: rgba(0, 0, 0, 0.7);
      color: white;
      padding: 5px 10px;
      border-radius: 5px;
      font-size: 12px;
    }

    .video-controls {
      display: flex;
      gap: 10px;
    }

    .video-controls button {
      padding: 12px 20px;
      border: none;
      border-radius: 8px;
      cursor: pointer;
      font-size: 14px;
      transition: background 0.3s;
    }

    .end-call {
      background: #dc3545;
      color: white;
    }

    .toggle-video,
    .toggle-audio {
      background: #6c757d;
      color: white;
    }

    .connection-status {
      margin-bottom: 10px;
      padding: 10px;
      border-radius: 5px;
      text-align: center;
      font-weight: bold;
    }

    .connecting {
      background: #fff3cd;
      color: #856404;
    }

    .connected {
      background: #d4edda;
      color: #155724;
    }

    .disconnected {
      background: #f8d7da;
      color: #721c24;
    }
  </style>
</head>

<body>
  <div class="container">
    <model-viewer
      id="avatar"
      field-of-view="10deg"
      animation-name="Wave"
      nimation-crossfade-duration="0.5"

      src="static/avatar2.glb"
      
      style="--poster-color: transparent;">
    </model-viewer>


    <div class="chat-section">
      <div class="messages" id="messages"></div>
      <div class="controls">
        <button id="recordBtn" class="record-btn">üéôÔ∏è Record</button>
        <button id="humanBtn" class="human-btn" style="display: none;">üë• Connect to Human</button>
      </div>
    </div>
  </div>

  <!-- Video Call Container (unchanged) -->
  <div class="video-container" id="videoContainer">
    <div class="connection-status" id="connectionStatus">Connecting to operator...</div>
    <div class="video-grid">
      <div class="video-box local-video">
        <video id="localVideo" autoplay muted></video>
        <div class="video-label">You</div>
      </div>
      <div class="video-box remote-video">
        <video id="remoteVideo" autoplay></video>
        <div class="video-label">Operator</div>
      </div>
    </div>
    <div class="video-controls">
      <button id="toggleVideo" class="toggle-video">üìπ Video On</button>
      <button id="toggleAudio" class="toggle-audio">üé§ Audio On</button>
      <button id="endCall" class="end-call">üìû End Call</button>
    </div>
  </div>
 
  


  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>
  <script>
    const recordBtn = document.getElementById("recordBtn");
    const humanBtn  = document.getElementById("humanBtn");
    const messagesEl = document.getElementById("messages");
    const avatar    = document.querySelector("#avatar");

    // =========== LIP-SYNC SETUP ===========
    let mouthMesh, audioCtx, analyser, dataArray;
    

  // ‚Äî‚Äî Replace the old load/polling block with this ‚Äî‚Äî
   // ‚Äî‚Äî Replace your old load-handler with this ‚Äî‚Äî
  

  // ‚Äî‚Äî NEW load+polling block ‚Äî‚Äî
  function setupMouthMesh() {
  if (!avatar) return;
  avatar.traverse(child => {
    if (child.isMesh && child.morphTargetDictionary) {
      const morphs = child.morphTargetDictionary;
      const names = Object.keys(morphs);
      let key = names.find(n => /mouth|open|viseme_aa/i.test(n));
      if (!key) {
        console.warn("No usable morph target found, using first:", names[0]);
        key = names[0];
      }
      mouthMesh = child;
      openIndex = morphs[key];
      console.log(`‚úÖ Using ${child.name} with morph "${key}" at index ${openIndex}`);
    }
  });
}


  // ‚Äî‚Äî UPDATED startLipSync ‚Äî‚Äî  
  function startLipSync(audio) {
    if (!mouthMesh) return;

    const audioCtx = new (window.AudioContext||window.webkitAudioContext)();
    const src     = audioCtx.createMediaElementSource(audio);
    const analyser= audioCtx.createAnalyser();
    analyser.fftSize = 128;
    src.connect(analyser);
    analyser.connect(audioCtx.destination);

    const data = new Uint8Array(analyser.fftSize);
    (function animateMouth(){
      analyser.getByteTimeDomainData(data);
      let sum = 0;
      for (let v of data) {
        const n = (v/128) - 1;
        sum += n*n;
      }
      const rms = Math.sqrt(sum/data.length);

      // use the dynamically-picked index
      mouthMesh.morphTargetInfluences[openIndex] = Math.min(rms * 2, 1);

      requestAnimationFrame(animateMouth);
    })();
  }


    // ======================================

    // =========== RECORD/TRANSCRIBE ===========
    let mediaRecorder, audioChunks = [];
    let audioCtxInput, analyserInput, micSource;   // for VAD
    recordBtn.onclick = async () => {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioChunks = [];
      mediaRecorder = new MediaRecorder(stream);
      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      mediaRecorder.onstop = () => processRecording();

      audioCtxInput = new AudioContext();
      micSource = audioCtxInput.createMediaStreamSource(stream);
      analyserInput = audioCtxInput.createAnalyser();
      micSource.connect(analyserInput);
      detectSilence();

      showLoading(true);
      mediaRecorder.start();
    };

    function detectSilence(threshold = 0.01, timeout = 1500) {
      const data = new Uint8Array(analyserInput.fftSize);
      let lastActivity = Date.now();
      (function loop(){
        analyserInput.getByteTimeDomainData(data);
        const silent = data.every(v=>Math.abs(v-128) < threshold*128);
        if (!silent) lastActivity = Date.now();
        if (Date.now() - lastActivity > timeout && mediaRecorder.state==="recording") {
          mediaRecorder.stop();
        } else {
          requestAnimationFrame(loop);
        }
      })();
    }

    function showLoading(loading) {
      if (loading) {
        recordBtn.disabled = true;
        recordBtn.innerHTML = `Listening... <span class="spinner"></span>`;
      } else {
        recordBtn.disabled = false;
        recordBtn.textContent = "üéôÔ∏è Record";
      }
    }
    // ========================================

    async function processRecording() {
      const audioBlob = new Blob(audioChunks, { type: "audio/mp3" });
      const formData = new FormData();
      formData.append("audio", audioBlob, "voice.mp3");

      // send to your /transcribe endpoint
      const trans = await fetch("http://localhost:8000/transcribe", { method:"POST", body:formData })
                        .then(r=>r.json());
      addMessage("user", trans.transcript);

      // send to /respond
      const respForm = new FormData();
      respForm.append("text", trans.transcript);
      const resp = await fetch("http://localhost:8000/respond", { method:"POST", body:respForm })
                       .then(r=>r.json());
      addMessage("assistant", resp.response);

      if (resp.needs_human_operator) {
        humanBtn.style.display = "block";
        addMessage("system",
          "I can connect you with a human operator‚Ä¶");
      }

      // === PLAY & LIP-SYNC ===
      const audio = new Audio("http://localhost:8000"+resp.audio_url);
      audio.addEventListener('play',()=> startLipSync(audio));
      audio.play();
      showLoading(false);
    }

    function addMessage(role,text) {
      const div = document.createElement("div");
      div.className = `message ${role}`;
      div.textContent = text;
      messagesEl.appendChild(div);
      messagesEl.scrollTop = messagesEl.scrollHeight;
    }

        // ====== RTC BUTTONS & ELEMENTS ======
    const videoContainer    = document.getElementById("videoContainer");
    const connectionStatus  = document.getElementById("connectionStatus");
    const localVideo        = document.getElementById("localVideo");
    const remoteVideo       = document.getElementById("remoteVideo");
    const toggleVideoBtn    = document.getElementById("toggleVideo");
    const toggleAudioBtn    = document.getElementById("toggleAudio");
    const endCallBtn        = document.getElementById("endCall");

    // ====== UTILS ======
    function generateUUID() {
      return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'
        .replace(/[xy]/g, c => (Math.random()*16|0).toString(16));
    }

    // ====== HUMAN BUTTON ======
    humanBtn.onclick = initializeVideoCall;

    // ====== WEBRTC STATE ======
    let localStream, peerConnection, signalingSocket;
    const clientId = generateUUID();
    let isVideoEnabled = true, isAudioEnabled = true;

    // ====== INITIALIZE VIDEO CALL ======
    async function initializeVideoCall() {
      try {
        localStream = await navigator.mediaDevices.getUserMedia({ video:true, audio:true });
        localVideo.srcObject = localStream;
        videoContainer.classList.add('active');
        setupPeerConnection();
        connectToSignalingServer();
        addMessage("system", "Connecting you to a human operator...");
      } catch (err) {
        console.error(err);
        addMessage("system", "Unable to access camera/microphone.");
      }
    }

    // ====== PEER CONNECTION ======
    function setupPeerConnection() {
      peerConnection = new RTCPeerConnection({
        iceServers: [
          { urls: 'stun:stun.l.google.com:19302' },
          { urls: 'stun:stun1.l.google.com:19302' },
          { urls: 'stun:stun2.l.google.com:19302' }
        ]
      });
      localStream.getTracks().forEach(track =>
        peerConnection.addTrack(track, localStream)
      );
      peerConnection.ontrack = e => {
        remoteVideo.srcObject = e.streams[0];
        connectionStatus.textContent = "Connected to operator";
        connectionStatus.className = "connection-status connected";
      };
      peerConnection.onicecandidate = e => {
        if (e.candidate && signalingSocket.readyState===WebSocket.OPEN) {
          signalingSocket.send(JSON.stringify({ type:'ice-candidate', candidate:e.candidate }));
        }
      };
      peerConnection.onconnectionstatechange = () => {
        const s = peerConnection.connectionState;
        if (s==='disconnected' || s==='failed') {
          connectionStatus.textContent = "Connection lost";
          connectionStatus.className = "connection-status disconnected";
          setTimeout(endVideoCall,2000);
        }
      };
    }

    // ====== SIGNALING ======
    function connectToSignalingServer() {
      const WS_BASE = window.location.protocol==='https:'?'wss:':'ws:';
      signalingSocket = new WebSocket(`${WS_BASE}//${window.location.host}/ws/signaling/${clientId}/customer`);
      signalingSocket.onopen = () => {
        connectionStatus.textContent = "Waiting for operator...";
        connectionStatus.className = "connection-status connecting";
      };
      signalingSocket.onmessage = async ({data}) => {
        const msg = JSON.parse(data);
        switch(msg.type) {
          case 'matched':
            connectionStatus.textContent = "Operator found! Establishing connection...";
            const offer = await peerConnection.createOffer({ offerToReceiveAudio:true, offerToReceiveVideo:true });
            await peerConnection.setLocalDescription(offer);
            signalingSocket.send(JSON.stringify({ type:'offer', offer }));
            break;
          case 'answer':
            await peerConnection.setRemoteDescription(msg.answer);
            break;
          case 'ice-candidate':
            await peerConnection.addIceCandidate(msg.candidate);
            break;
          case 'partner_disconnected':
            connectionStatus.textContent = "Operator disconnected";
            connectionStatus.className = "connection-status disconnected";
            setTimeout(endVideoCall,2000);
            break;
        }
      };
      signalingSocket.onerror = e => {
        console.error(e);
        connectionStatus.textContent = "Connection error";
        connectionStatus.className = "connection-status disconnected";
      };
      signalingSocket.onclose = () => {
        if (videoContainer.classList.contains('active')) {
          connectionStatus.textContent = "Connection lost";
          connectionStatus.className = "connection-status disconnected";
        }
      };
    }

    // ====== VIDEO CONTROLS ======
    toggleVideoBtn.onclick = () => {
      isVideoEnabled = !isVideoEnabled;
      localStream.getVideoTracks().forEach(t=>t.enabled=isVideoEnabled);
      toggleVideoBtn.textContent = isVideoEnabled?"üìπ Video On":"üìπ Video Off";
    };
    toggleAudioBtn.onclick = () => {
      isAudioEnabled = !isAudioEnabled;
      localStream.getAudioTracks().forEach(t=>t.enabled=isAudioEnabled);
      toggleAudioBtn.textContent = isAudioEnabled?"üé§ Audio On":"üé§ Audio Off";
    };
    endCallBtn.onclick = endVideoCall;

    function endVideoCall() {
      if (peerConnection) { peerConnection.close(); peerConnection=null; }
      if (signalingSocket && signalingSocket.readyState===WebSocket.OPEN) {
        signalingSocket.send(JSON.stringify({type:'disconnect'}));
        signalingSocket.close();
      }
      if (localStream) {
        localStream.getTracks().forEach(t=>t.stop());
        localStream=null;
      }
      videoContainer.classList.remove('active');
      humanBtn.style.display = "none";
      addMessage("system", "Video call ended. You can continue chatting.");
    }

    // ====== CLEANUP ON UNLOAD ======
    window.addEventListener('beforeunload', () => {
      if (peerConnection||signalingSocket) endVideoCall();
    });

    // ‚úÖ Define addMessage function
    function addMessage(role, text) {
      const messagesEl = document.getElementById("messages");
      const div = document.createElement("div");
      div.className = `message ${role}`;
      div.textContent = text;
      messagesEl.appendChild(div);
      messagesEl.scrollTop = messagesEl.scrollHeight;
    }

    recordBtn.onclick = async () => {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      audioChunks = [];
      mediaRecorder = new MediaRecorder(stream);

      mediaRecorder.ondataavailable = e => audioChunks.push(e.data);
      mediaRecorder.onstop = () => processRecording();

      mediaRecorder.start();
      setTimeout(() => mediaRecorder.stop(), 3000);
    };

    async function processRecording() {
      const audioBlob = new Blob(audioChunks, { type: "audio/mp3" });
      const formData = new FormData();
      formData.append("audio", audioBlob, "voice.mp3");

      console.log("Sending audio to /transcribe...");
      const trans = await fetch("/transcribe", {
        method: "POST",
        body: formData
      }).then(res => res.json()).catch(err => {
        console.error("/transcribe error:", err);
      });

      console.log("Transcript:", trans);
      if (!trans || !trans.transcript) {
        addMessage("system", "‚ö†Ô∏è No transcription received.");
        return;
      }
      addMessage("user", trans.transcript);

      const checkInterruptForm = new FormData();
      checkInterruptForm.append("audio", audioBlob, "voice.mp3");
      const interrupt = await fetch("/interrupt-check", {
        method: "POST",
        body: checkInterruptForm
      }).then(res => res.json()).catch(() => ({ interrupt: false }));

      if (interrupt.interrupt && currentAudio) {
        currentAudio.pause();
        addMessage("system", "üîá Audio playback interrupted.");
        return;
      }

      const respForm = new FormData();
      respForm.append("text", trans.transcript);
      console.log("Sending transcript to /respond...", trans.transcript);

      const resp = await fetch("/respond", {
        method: "POST",
        body: respForm
      }).then(res => res.json()).catch(err => {
        console.error("/respond error:", err);
      });

      console.log("Bot reply:", resp);
      if (!resp || !resp.response) {
        addMessage("system", "‚ö†Ô∏è Bot failed to respond.");
        return;
      }
      addMessage("assistant", resp.response);

      if (resp.needs_human_operator) {
        humanBtn.style.display = "block";
        addMessage("system", "Want to speak to a human?");
      }

      if (currentAudio) currentAudio.pause();
      currentAudio = new Audio(resp.audio_url);
      console.log("Playing audio:", resp.audio_url);
      currentAudio.play();
    }
  </script>
</body>
</html>
